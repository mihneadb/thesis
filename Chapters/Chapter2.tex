% Chapter Template

\chapter{Background and Related Work} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 2. \emph{Background and Related Work}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title


As we outlined in the Introduction, distributed MTC filesystems are inherently different when compared to usual filesystems. Thus, they have to be benchmarked differently. The relevant set of metrics that needs to be measured is defined in the MTC Envelope\cite{envelope}. It includes the following:

\begin{itemize}

\item file open operation throughput
\item file creation operation throughput
\item 1-to-1 read data throughput
\item 1-to-1 read data bandwidth
\item N-to-1 read data throughput
\item N-to-1 read data bandwidth
\item write data throughput
\item write data bandwidth

\end{itemize}

In the above list, we can observe two special access patterns, specific to MTC filesystems: 1-to-1 and N-to-1 reads.

The 1-to-1 access pattern can be described as follows. There is created a number \textit{N} of files. Then, the \textit{M} nodes each read $N / M$ of the files. Thus, every node has to do with its own set of files and there is no concurrent access over any one file.

The N-to-1 access pattern is a bit more involved. There is only one file created, and then all \textit{M} nodes read it, concurrently. This pattern generates contention and is more penalizing on the performance of the filesystem.

Overall, there are already a significant number of filesystem benchmarks available\cite{fsbench}. However, none of them is a good fit on its own for benchmarking MTC filesystems. We outline a few sample reasons.

Most filesystem benchmarks target disk-based filesystems. This does not fit MTC filesystems because they use RAM as datastore, for performance reasons. The differences between RAM and disks make regular benchmarks unreliable. For example, memory is random access, while disk drives favor sequential access. The difference in access patterns yields less relevant results overall.

Generally, filesystem benchmarks are local - they run on a single node, just like the filesystem implementations they commonly test. This means that in order to run a benchmark on a distributed filesystem, one has to copy the executable on all nodes, run it simultaneously and then aggregate all the results. This approach is tedious, requiring a lot of extra work that is unrelated to the research problem. Furthermore, this way of running tests is not necessarily correct. For example, if an N-to-1 pattern is tested, the N readers should access the file all at the same time. To obtain this, a global synchronization mehtod, such a barrier, should be used. This synchronization cannot be reached by just using a shell script. This means that different test runs will output different results.

However, we can build upon such existing benchmarks. For our solution, we chose to use \textit{IOzone}\cite{iozone} and \textit{mdtest}\cite{mdtest}.


IOzone is a benchmark for measuring I/O performance. It can generate multiple types of file operations, as well as measure their performance. The main operations that are useful for our purposes are:

\begin{itemize}
\item read
\item write
\item re-read
\item re-write
\end{itemize}

Other operations like random read and random write are supported as well, but they are not relevant for memory-based filesystems.

A big advantage of IOzone is that it is written in ANSI C, using POSIX APIs. Because of this, IOzone has been ported to run on multiple computer platforms. It also supports large files in its test runs, which helps in simulating MTC-sized data files.

Another useful feature of IOzone that made embedding it into our benchmark easier is the fact that it can output its results in tab-separated columns, making the results easy to parse.

Below are a subset of the configuration options IOzone supports.

\begin{itemize}

\item cache line size
\item total cache size
\item record size
\item file size
\item including the \textit{close} operation in the timing
\item including flush operations (\textit{fsync}, \textit{fflush}) in the timing
\item performing synchronized writes
\item give results in operations per second
\item purging the CPU cache before each operation

\end{itemize}

While IOzone is a great tool for benchmarking the throughput of reads and writes, we turned to mdtest for benchmarking metadata operations. \textit{mdtest}\cite{mdtest} is a filesystem metadata benchmark that works as follows - first, it creates a file structure (directories and trees) and then it measures performance of operations like \textit{open}, \textit{stat} and \textit{close} on that file structure.

\textit{mdtest} is highly configurable, supporting options like:

\begin{itemize}

\item branching factor of the created file tree
\item number of iterations
\item number of files created within a directory
\item restrict to only measuring one operation (i.e. read)

\end{itemize}

On top of these, mdtest is already a distributed benchmark through the use of MPI. This means that we can use mdtest out of the box to benchmark metadata operations on MTC filesystems. However, there is still room for improvement, especially in the results aggregation and plotting. The benchmark's results are not presented in a machine-readable format, and that means there is no easy way to feed them into a plotting tool. To overcome this, one can use a parser that transforms mdtest output into structured data, like a JSON file.
