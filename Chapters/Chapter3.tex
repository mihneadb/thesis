% Chapter Template

\chapter{A Benchmark for MTC Filesystems} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 3. \emph{A Benchmark for MTC Filesystems}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

In the previous two chapters we have discussed about Many-Task Computing, how it is relevant for today's  computations and why it needs special filesystems. We looked at how those filesystems differ from regular disk-based ones, as well as why they need to be benchmarked differently. We also outlined the shortcomings of current benchmarks and how they can be surpassed. In this chapter, we focus on our approach to building an MTC-specific filesystem benchmark, the architecture behind it and the ways in which it can be used.

\section{Running coordinated commands across multiple nodes}

As we previously explained, in order to correctly benchmark access patterns on distributed filesystems, all nodes have to start running any given operation simultaneously. To solve this, we need to have a way of synchronizing nodes. Fortunately, synchronization is a common problem with multiple solutions in distributed programming frameworks.

To solve this problem, we used OpenMPI\cite{openmpi} barriers. We developed an MPI application that executes any given command on a set of available nodes. To do so, we spawn MPI workers in the default shared MPI communicator, we then synchronize them with the use of a barrier, and then the workers run the given command. By using the barrier, we can be sure that the command is run in a coordinated manner, at the same time.

% TODO - diagram with MPI workers sync by barrier + exec cmd


% ---------------

\section{Aggregating the outputs of multiple nodes}


% TODO - diagram with local execution, redirect to file, sending data from file
% TODO - diagram with slave nodes and master sending back output

% --------------


\section{Why OpenMPI}

** support standard MPI
** cluster agnostic

% ----------

\section{Varying the number of nodes}

** python wrapper for the discussed mpi code
** machinefiles trick
** reservation separated for easy supporting different clusters

* automate the cluster reservation process

% ----------

\section{Leveraging existing benchmarks}

\subsection{IOzone}

% --------

\subsection{mdtest}

% --------




\subsection{Producing structured results}


% ---------

\subsection{Plotting the results}


% --------

% ---------------


* specifying complex tests

** architecture

